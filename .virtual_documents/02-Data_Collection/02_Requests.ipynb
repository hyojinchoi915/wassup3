pip install requests





https://finance.naver.com/


import requests
from bs4 import BeautifulSoup


PATH = "https://finance.naver.com/"
resp = requests.get(PATH)


resp


resp?


requests.get?


resp.text


print(resp.text)


## 데이터를 추출하기 위해서는 DOM 객체로 변환해야한다.



src = resp.text


soup = BeautifulSoup(src, 'lxml')


quiz : 주요뉴스의 목록을 추출해보자


srclist = soup.select('.section_strategy li a') #li를 생략하면 제목(주요뉴스 더보기)까지 같이 나옴
srclist


# 첫번째 뉴스 제목 추출
srclist[0].text


srclist[0]['href']





url = srclist[0]['href']
PATH + url


from urllib.parse import urljoin


urljoin(PATH, url)


# quiz : 네이버 증권 주요뉴스의 제목과 url 모두 추출하기


# 제목 변수, url 변수 만들기

news_title = []
news_url = []
for item in srclist :
    title = item.text
    url = urljoin(PATH, item['href'])
    news_title.append(title)
    news_url.append(url)





import pandas as pd


df = pd.DataFrame({'제목':news_title, '주소':news_url})
df





df.to_excel('output/naver.xlsx', index = False)


# index 넣어주는게 자동임





# 파일명에 저장시각을 추가해보자.
import time


today = time.localtime()
today


df.to_excel(f'output/{today.tm_year}_{today.tm_mon}_{today.tm_mday}naver.xlsx', index = False)


# 스트링 포매팅으로 날짜 포맷을 바꿔보자.
# 2024-06-20
'%d-%02d-%02d'%(today.tm_year, today.tm_mon, today.tm_mday)


filename = '%d-%02d-%02d'%(today.tm_year, today.tm_mon, today.tm_mday)
excel_name = filename + '.xlsx'
csv_name = filename + '.csv'
excel_name, csv_name


df.to_csv(f'output/{csv_name}')





https://www.melon.com/chart/index.htm


#컴퓨터를 새로 켰으니 다시 임포트해주자
import requests
from bs4 import BeautifulSoup
import pandas as pd


# PATH를 지정해줍니다. 
PATH = 'https://www.melon.com/chart/index.htm'
resp = requests.get(PATH)
resp


resp.text


# 엥 왜 안나오지? 유저 에이전트 등록을 안해서 그럼.
유저 에이전트 정보를 받아올 수 있는 방법은? 크롬에서..chrome://version/ 이렇게 치면 사용자 에이전트가 나온다.
혹은 f12 눌러 네트워크>헤더에 있기도 함


info = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'}
info


resp = requests.get(PATH, headers = info)
resp


html_src = resp.text
html_src


# 텍스트로 받아는 왔는데 보기 불편하니 돔객체로 변환해주자
soup = BeautifulSoup(html_src, 'lxml')
soup


#곡이름 찾기
# soup.select('.ellipsis > span > a')[0].text
soup.select('.ellipsis.rank01 a')


song = {item.text for item in soup.select('.ellipsis.rank01 a')}
len(song)


#가수 이름 찾기
artist = []
for item in soup.select('.ellipsis.rank02>a'):
    artist.append(item.text)
print(len(artist))
print(artist)  # 탑 100인데 106개가 나오는 이유는 여러 아티스트가 부른 노래가 있어서


soup.select('.checkEllipsis')
len(soup.select('.checkEllipsis'))


name = {item.text for item in soup.select('.checkEllipsis')}
len(name)


#순위 인덱스 만들기
rank = list((range(1, 101)))
len(rank)


# 판다스로 데이터프레임 만들기 pd.Dataframe({}) 이렇게 만들고 안에 키와 밸류로 데이터를 넣어주면 된다.
songDF = pd.DataFrame({'순위': rank,
             '노래제목': song,
             '가수명': name})
songDF





songDF.to_excel('output/melon100.xlsx', index = False)





https://blog.naver.com/gikimirane/222666913565
https://resttesttest.com/


import requests


param = {'name' : 'Amy', 'age':20, 'address':'Seoul'}


#get방식
resp1 = requests.get('http://httpbin.org/get', params = param)
print(resp1)
print(resp1.text)


# POST방식 파라미터가 아니라 데이터로 받아와서 params가 아니라 data로 넣어야한다.
resp2 = requests.post('http://httpbin.org/post', data = param)
print(resp2)
print(resp2.text)
# 값이 form에 들어와있음 form이 무엇이냐? 입력input요소들을 form이라고 하는듯? 그래서 여기 입력한 것만 post로 받아옴





https://search.naver.com/search.naver?sm=tab_hty.top&where=nexearch&ssc=tab.nx.all&query=ai


from bs4 import BeautifulSoup


#기준으로 앞쪽이 도메인
PATH = 'https://search.naver.com/search.naver'


keyword = input('검색어를 입력하세요:')


resp = requests.get(PATH, params = {'query':keyword})
resp


html_src = resp.text
html_src


soup = BeautifulSoup(html_src, 'lxml')
soup


result = soup.select('.news_tit')


for a in result:
    print(a.text, a['href'])





https://korean.visitkorea.or.kr/


from bs4 import BeautifulSoup


PATH = 'https://korean.visitkorea.or.kr/'


resp = requests.get(PATH)
resp


resp.text


html_src = resp.text
html_src


soup = BeautifulSoup(html_src, 'lxml')
soup


result = soup.select('.popular')
print(result) #안나오는 이유는 정적 웹페이지라서.
