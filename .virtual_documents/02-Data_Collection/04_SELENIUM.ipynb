


# 셀레니움 라이브러리 설치
!pip install selenium


from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time

# Options라는 객체를 option에 넣어둘것
options = Options()
options


options.add_experimental_option("detach", True) #특정함수안에서  드라이버 생성시 함수종료될 때 브라우저 같이 종료되는 문제 대응





url = 'http://naver.com'
driver = webdriver.Chrome(options=options)


#실행된 드라이버에 url을 넣어보자
driver.get(url)
time.sleep(2)





driver.back()


driver.forward()


driver.refresh()





title = driver.title # 탭에 대한 타이틀
url = driver.current_url #현재 도메인 주소
handle = driver.current_window_handle #윈도우 창에 대한 ID ->ID가 있다는 건 동시에 여러 개를 핸들링 할 수도 있음 
print(title, url, handle)





driver.find_element?


# f12를 눌러서 검색창의 태그 요소를 찾아보자.
<input id="query" name="query" type="search" title="검색어를 입력해 주세요." placeholder="검색어를 입력해 주세요." maxlength="255" autocomplete="off" class="search_input" data-atcmp-element="">


driver.find_element(By.ID, 'query')


# 위에서 찾아준 객체에 키를 입력해주자.
# 객체는 항상 메서드를 가지고 있음
driver.find_element(By.ID, 'query').send_keys('뉴진스')
#검색창에 뉴진스가 들어간다!


driver.find_element(By.CLASS_NAME, 'search_input').send_keys('블랙핑크')


driver.find_element(By.NAME, 'query').send_keys('르세라핌')


#CSS 셀렉터로도 가능하다.
driver.find_element(By.CSS_SELECTOR, '#query').send_keys('에스파')


driver.find_element(By.CSS_SELECTOR, '.search_input').send_keys('세븐틴')


driver.find_element(By.CSS_SELECTOR, "[title='검색어를 입력해 주세요.']").send_keys('트와이스')


#이도저도 아닐땐 xpath -> f12로 찾아놓은 태그에 우클릭 >copy >copy xpath
driver.find_element(By.XPATH, '//*[@id="query"]').send_keys('BTS')





driver.find_element(By.LINK_TEXT, '쇼핑')
# 해당 텍스트가 있는 객체를 찾아줘


driver.find_element(By.LINK_TEXT, '쇼핑').click()


# 일부 매칭
driver.find_element(By.PARTIAL_LINK_TEXT, '증').click()


# 태그로 찾기
# 태그는 요소가 너무 많으므로 정확하게 대상을 찾을 때에는 권장 안함
driver.find_element(By.TAG_NAME, 'div')


# 여러 개의 요소를 찾을 때
driver.find_elements(By.CSS_SELECTOR, '.link_service')


# 첫 번째 요소 뽑아오기
driver.find_elements(By.CSS_SELECTOR, '.link_service')[0].get_attribute('href')


list = driver.find_elements(By.CSS_SELECTOR, '.link_service')
for a in list :
    print(a.get_attribute('href'))





# 테스트용 html
url = 'file:///C:/workspace/wassup3/02-Data_Collection/sample/signin.html'
driver = webdriver.Chrome(options=options)
driver.get(url)
time.sleep(2)


# ID에 코리아라고 입력해보기
username = driver.find_element(By.NAME, 'username')
username.send_keys('korea')


password = driver.find_element(By.NAME, 'password')
password.send_keys('1234')


# 클릭
login = driver.find_element(By.XPATH, '//*[@id="loginForm"]/input[3]')
login.click()


driver.back()


#클릭을 css 셀렉터로 해보기
login = driver.find_element(By.CSS_SELECTOR, '[value=Login]')
login.click()


driver.back()


username.clear()


password.clear()


#submit이 있어서 간단하게 이렇게 해줄수도 있음
username.submit()


driver.back()


driver.find_element(By.TAG_NAME, 'p').text


# html 소스 추출하기
driver.page_source


driver.close()





from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time

options = Options()
options.add_argument("--headless=new") 
# options.add_argument('--window-size= x, y') #실행되는 브라우저 크기를 지정할 수 있습니다.
# options.add_argument('--start-maximized') #브라우저가 최대화된 상태로 실행됩니다.
# options.add_argument('--start-fullscreen') #브라우저가 풀스크린 모드(F11)로 실행됩니다.
# options.add_argument('--blink-settings=imagesEnabled=false') #브라우저에서 이미지 로딩을 하지 않습니다.
# options.add_argument('--mute-audio') #브라우저에 음소거 옵션을 적용합니다.
# options.add_argument('incognito') #시크릿 모드의 브라우저가 실행됩니다.
options.add_experimental_option("detach", True) #특정함수안에서  드라이버 생성시 함수종료될 때 브라우저 같이 종료되는 문제 대응


url = 'http://naver.com'
driver = webdriver.Chrome(options=options)
driver.get(url)
time.sleep(2)


#실행된지 모르겠음 프린트해보자. 잘된다... headless가 바로바로 UI 없이 백그라운드에서만 작동하는
print(driver.title)


driver.quit() # 탭 닫기 -> close는 브라우저를 종료할것이냐 quit은 탭을 종료할것이냐의 차이





from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time

options = Options()
# options.add_argument("--start-maximized")
# options.add_argument("--headless=new") 
options.add_experimental_option("detach", True) #

url = 'http://naver.com'
driver = webdriver.Chrome(options=options)
driver.get(url)
time.sleep(2)


#창의 너비/높이 구하기
size = driver.get_window_size()
width = size.get("width")
height = size.get("height")

print(str(width)+"px"+" "+str(height)+"px")


# 창 크기 조절
driver.set_window_size(800, 600)


# 스크린 상에서의 창 좌표
position = driver.get_window_position()
x = position.get('x')
y = position.get('y')

print("x : "+str(x)+" "+"y : "+str(y))


driver.set_window_position(200,200)


# 창 크기 최대화
driver.maximize_window()


# 창 크기 최소화
driver.minimize_window()


# 전체 화면
driver.fullscreen_window()


# 스크린 샷
driver.save_screenshot('./image.png')


driver.close()





from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time, random, pandas as pd

options = Options()
options.add_argument('--window-size=974,1047')
options.add_argument('--window-position=-7,0')
options.add_experimental_option("detach", True)


# 웹드라이버 로드
driver = webdriver.Chrome( options = options)


# URL 조건 설정
where = 'blog'
query = '인공지능'
dateform = '20240101to20240625'
url = f'https://search.naver.com/search.naver?ssc=tab.{where}.all&query={query}&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom{dateform}'
# url = f'https://search.naver.com/search.naver?where={where}&query={query}&sm=tab_op&nso=so:r,p:from{dateform}'
fname = f'{where}_{query}_{dateform}'


# url 접속
driver.get(url)
time.sleep(random.randint(2,3))


하지만 이렇게 해봤자 몇 개 못가져옴
이유는 해당 페이지가 동적이기 때문에..
그래서 자바스크립트 함수를 써야한다.





#네이버 뷰는 최대 1050까지만 노출
# 스크롤 10번
for i in range(10):
    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')
    time.sleep(random.randint(2, 3))


이제 Css html 정보를 클라이언트가 받아온 상태가 되었다.
이제 css 셀렉터를 쓸 수 있따.


# get_view()
views = driver.find_elements(By.CSS_SELECTOR, '.lst_view .view_wrap')
result = []

for view in views:
    con_dict = {}
    con_dict['title'] = view.find_element(By.CSS_SELECTOR, '.title_link').text
    con_dict['text'] = view.find_element(By.CSS_SELECTOR, '.dsc_link').text
    con_dict['date'] = view.find_element(By.CSS_SELECTOR, '.sub').text
    result.append(con_dict)
    print(con_dict)
    
print('완료')


#저장된 게시물 리스트를 데이터프레임으로 변환후 csv로 저장
df = pd.DataFrame(result)
df


#데이터 프레임 저장
# csv는 sep 쉼표가 반드시 있어야 한글이 안깨짐
df.to_csv(f'./naver_{fname}.csv', sep=',', encoding='utf-8-sig')





from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time, random, pandas as pd

options = Options()
options.add_argument('--window-size=974,1047')
options.add_argument('--window-position=-7,0')
options.add_experimental_option("detach", True)

driver = webdriver.Chrome( options = options)
url = 'https://play.google.com/store/apps/details?id=com.estsoft.picnic'
driver.get(url)
time.sleep(random.randint(2,3))


# 리뷰 모두 보기 클릭
reviewer = driver.find_element(By.XPATH, '//*[@id="ow76"]/section/div/div[2]/div[5]/div/div/button/span')
reviewer.click()


# 스크롤
for i in range(10):
    review_box = driver.find_element(By.CSS_SELECTOR, 'div.fysCi')
    driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', review_box )
    time.sleep(random.randint(2, 3))


reviews = driver.find_elements(By.CSS_SELECTOR, 'div.RHo1pe')
reviews


# 요소별 추출
def get_content(review):
    condic = {}
    condic['text'] = review.find_element(By.CSS_SELECTOR, 'div.h3YV2d').text
    condic['rat'] = len(review.find_elements(By.CSS_SELECTOR, 'span.Z1Dz7b'))
    condic['date'] = review.find_element(By.CSS_SELECTOR, 'span.bp9Aid').text
    return condic
print(condic)





from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import math, time

options = Options()
options.add_argument('--window-size=974,1047')
options.add_argument('--window-position=-7,0')
options.add_experimental_option("detach", True)


search = input('검색어를 입력하세요.')


URL = 'https://korean.visitkorea.or.kr/search/search_list.do?keyword='+search

driver = webdriver.Chrome(options=options)
driver.get(URL)
time.sleep(3)


## 더보기 클릭
deobogi = driver.find_element(By.CSS_SELECTOR, '#s_recommend > .more_view > a')
deobogi.click()


## 타이틀 뽑아오기
title = driver.find_elements(By.CSS_SELECTOR, ".tit a")
title[0].text


title = driver.find_elements(By.XPATH, '//*[@id="search_result"]/ul/li[1]/div[1]/div[1]/a')
title[0].text


# tit_xpath = '//*[@id="search_result"]/ul/li/div/div/a' # 해당 요소가 하나씩일 때
tit_xpath = '//*[@id="search_result"]/ul/li[*]/div[1]/div[1]/a'
result = driver.find_elements(By.XPATH, tit_xpath)
len(result), result[0].text, result[1].text


for i, title in enumerate(result, 1):
#     print(i, title.text)
    print(i, title.text)





# 2번 페이지 버튼 클릭
pclick = driver.find_element(By.XPATH, '/html/body/div[3]/div/div[1]/div[14]/a[2]')
pclick.click()


cnt = int(input('크롤링 할 건수는 몇건입니까?: '))
cnt


# 우리는 몇페이지까지 가야되냐? 반올림해보자
page_cnt = math.ceil(cnt/10)
print(page_cnt)





# 페이지 당 10개 게시물이 있다. 우리가 페이지를 순환해야 하기 때문에. 가장 먼저 해야할 것은 '페이지 순환'이다.
# 그래서 page_cnt 받아온 것에 대해 for 문을 돌리는 것이다. 

tit_xpath = '//*[@id="search_result"]/ul/li[*]/div[1]/div[1]/a'
no = 0

for x in range(1, page_cnt+1):
    print(f'========= {x} 페이지 작업 =========')
    mylist = driver.find_elements(By.XPATH, tit_xpath)  # 1페이지는 이미 눌려져 있어서 돌릴 필요 없음. 먼저 뽑아준거
    
    for item in mylist:
        no += 1  
        if no > cnt:
            break
        print(no, item.text)
    
    if no <= cnt:
        a = f'/html/body/div[3]/div/div[1]/div[14]/a[{x+1}]'
        driver.find_element(By.XPATH, a).click()
#         next_button = driver.find_element(By.CSS_SELECTOR, f"a[id='{x+1}']")
#         driver.execute_script("arguments[0].click();", next_button)
        time.sleep(2)
    
print('========= 작업 완료 =========')
# driver.close()


# 페이지 순환 프로그램 만들기(6p를 넘어가 끝까지 수집하는) 

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import math, time

options = Options()
options.add_argument('--window-size=974,1047')
options.add_argument('--window-position=-7,0')
options.add_experimental_option("detach", True)


search = input('검색어를 입력하세요.')


URL = 'https://korean.visitkorea.or.kr/search/search_list.do?keyword='+search

driver = webdriver.Chrome(options=options)
driver.get(URL)
time.sleep(3)


deobogi = driver.find_element(By.CSS_SELECTOR, '#s_recommend > .more_view > a')
deobogi.click()





from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import math, time

options = Options()
options.add_argument('--window-size=974,1047')
options.add_argument('--window-position=-7,0')
options.add_experimental_option("detach", True)


URL = 'https://korean.visitkorea.or.kr/detail/rem_detail.do?cotid=be3db10c-b642-409c-81cc-c4cdecb5bd8b&temp='

driver = webdriver.Chrome(options=options)
driver.get(URL)
time.sleep(3)


# 제목 추출
title = driver.find_element(By.ID, 'topTitle').text
title


# 내용 추출
contents = driver.find_elements(By.CLASS_NAME, 'txt_p')

contents_list = []
for item in contents:
    contents_list.append(item.text)
contents_list


# 하나의 문자열로 통합 -> 텍스트를 합쳐주는 조인 함수 쓰기
contents_merge = ' '.join(contents_list) 
contents_merge





from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
import time, urllib.request

options = Options()
options.add_argument('--window-size=974,1047')
options.add_argument('--window-position=-7,0')
options.add_experimental_option("detach", True)


# 가장 먼저 할것은 이미지를 저장할 디렉토리를 만드는 것


pwd


import os
os.getcwd()


f_dir = input('이미지 저장 폴더명 : ')

now = time.localtime()
s = '%04d%02d%02d_%02d%02d%02d'%(now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)
f_name = f_dir +'_' + s

os.makedirs(os.getcwd()+'\\' + f_name)


#디렉토리가 준비되었으니 이미지를 추출해보자.
# 참고로 이미지는 정적 웹페이지여도 스크롤을 미리 안해준다면 하나만 불러옴


driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")


html_src = driver.page_source


html_dom = BeautifulSoup(html_src, 'lxml')
html_dom


#이미지 소스만 뽑아오는 코드를 찾아보자.

mylist = html_dom.select('.img_typeBox img')
mylist


img_list = [item['src'] for item in mylist]
img_list


urllib.request.urlretrieve(img_list[0],  'image.jpg')


# 인덱스.. 이렇게 해도 되고 enumerate 해줘도 됨

no = 0
for src in img_list:
    # 다운로드  (주소, 파일이름)
    urllib.request.urlretrieve(src, f'{f_name}\\{no}.jpg')
    no += 1








from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options

options = Options()
options.add_argument('--window-size=974,1047')
options.add_argument('--window-position=-7,0')
options.add_experimental_option("detach", True)


import os
f_dir = 'stock' #stock 이라는 디렉토리 경로를 만들어준거
os.makedirs(os.getcwd()+'\\' + f_dir)

driver = webdriver.Chrome(options=options)
URL = 'http://data.krx.co.kr/contents/MDC/MDI/mdiLoader/index.cmd?menuId=MDC0301'  
driver.get(URL)


# 주식 클릭 > 상장종목 주요기록 클릭하도록 만들어보자.
driver.find_element(By.LINK_TEXT, '주식').click()
driver.find_element(By.LINK_TEXT, '상장종목 주요기록').click()


driver.find_element(By.LINK_TEXT, '주식').click()





driver.find_element(By.XPATH, '//*[@id="mktId_0_1"]').click() #< 이거 안됨 js함수를 써줘야함


market = driver.find_element(By.ID, 'mktId_0_1')
driver.execute_script('arguments[0].click()', market)





idx1 = int(input(' 0.전체      1.KOSPI     2.KOSDAQ    3.KONEX  ==> 시장 번호 선택:  '))
idx2 = int(input(' 0.Excel 파일로 저장      1. CSV 파일로 저장   ==> 파일 형식 선택:  '))


market_type = ['mktId_0_0','mktId_0_1', 'mktId_0_2', 'mktId_0_3']
market = driver.find_element(By.ID, market_type[idx1])


driver.execute_script('arguments[0].click()', market)
market.is_selected()


# 선택이 되어ㅣㅇㅆ으면 true


# 조회 버튼 클릭하기
driver.find_element(By.ID, 'jsSearchButton').click()


# 다운버튼 클릭
driver.find_element(By.CLASS_NAME, 'CI-MDI-UNIT-DOWNLOAD').click()


# 파일 유형 클릭하기
file_type =['Excel', 'CSV'] # 위에 만들어준 input에 1,2 넣는 것을 활용하기 위해 file_type이라는 리스트를 만들어주는 것
driver.find_element(By.LINK_TEXT, file_type[idx2]).click()


driver.close()



