


## JSON 활용 연습
http://www.krei.re.kr:18181/chart/main_chart/index/kind/W/sdate/2019-01-01/edate/2019-12-31


import requests, json, pandas as pd #api는 정적이라 request 쓰면된다


URL ='http://www.krei.re.kr:18181/chart/main_chart/index/kind/W/sdate/2019-01-01/edate/2019-12-31'
resp = requests.get(URL)
resp


data = resp.text
print(type(data), len(data))
print(data)


#보기 힘드니까 json으로 변환시켜보자. 텍스트로 받아오긴 했는데 딕셔너리 구조의 파일이니깐


data2 = resp.json()
print(type(data2), len(data2))
print(data2)


data2


id_, date, symbol, open_, close= [], [], [], [], []

for item in data2 :
    id_.append(item['id'])
    date.append(item['date'])
    symbol.append(item['symbol'])
    open_.append(item['open'])
    close.append(item['close'])


df = pd.DataFrame({
    'id' : id_,
    'date' : date,
    'symbol' :symbol,
    'open' : open_,
    'close' : close,
})





import requests
from bs4 import BeautifulSoup


# http://openapi.molit.go.kr:8081/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTrade?LAWD_CD=11110&DEAL_YMD=201512&serviceKey=6Ky9r%2FBFwB03VpfvW9HaG2XUHdaT8Bgf%2FSYbpgUWUdPP871hjIoXER6Q0KgIARbZFMaRcv8oYp2dajZQax0p1g%3D%3D
date = '201512'
region_code = '11110'
key = '6Ky9r%2FBFwB03VpfvW9HaG2XUHdaT8Bgf%2FSYbpgUWUdPP871hjIoXER6Q0KgIARbZFMaRcv8oYp2dajZQax0p1g%3D%3D'
url = f'http://openapi.molit.go.kr:8081/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTrade?LAWD_CD={region_code}&DEAL_YMD={date}&serviceKey={key}'

resp = requests.get(url)
textsrc = resp.text
textsrc


soup = BeautifulSoup(textsrc, 'xml')
soup #뷰티풀수프를 사용해서 xml형태로 변환


items = soup.select('response > body > items > item')
items


items[0]


a = items[0].select('거래금액')
print(type(a), a)


a = items[0].select_one('거래금액')
print(type(a), a)


a.text


a.text.strip()


for item in items:
    print(item.select_one('거래금액').text.strip(),
          item.select_one('건축년도').text,
          item.select_one('년').text,
          item.select_one('법정동').text.strip(),
          item.select_one('아파트').text,
          item.select_one('월').text,
          item.select_one('일').text,
          item.select_one('전용면적').text,
          item.select_one('지번').text,
          item.select_one('지역코드').text,
          item.select_one('층').text)





items = soup.select('response > body > items > item')
items














import requests, time, os, json
from html import unescape


# input : 나는 전주여행과 경주여행에 대해 정보를 가져오고 싶어, 그리고 목표 페이지는 5ㅍㅔ이지야
client_id = 've7cfMbnwwUb1RzCEHj4'
client_secret = 'i8GhJ9BiFa'

queries = ['전주 여행', '경주 여행']
goal_page = 5


# setting - 유저 에이전트를 입력해줘야 한다.
user_agent = "Mozilla/5.0 (Windows NT 10.0; WOW64) " + \
             "AppleWebKit/537.36 (KHTML, like Gecko) " + \
             "Chrome/51.0.2704.103 Safari/537.36"
# 요청할 헤더값을 딕셔너리 형태로 넣어놨
headers = {"User-Agent": user_agent,
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret}


#이런 경로에 파일을 만들어놓을거야. (만든 파일 확인해보면 컬럼명이 미리 만들어져있음 -> 데이터를 수집할 때.. 중간에 오류나면 노답이라서 중간마다 계쏙 저장해줘야 하기 때문)

file_name = 'output/naver_kin.txt'

with open(file_name, 'w', encoding='utf-8') as f :
    f.write('query\tno\ttitle\tlink\tdescription\ttotal_text\n')


# 준비는 다 되었다 네이버에 접속!

url = "https://openapi.naver.com/v1/search/kin.json?display=100&query=" + queries[0] + "&start=" + str(1)
response = requests.get(url, headers=headers)
response

#queries인 이유는 전주, 경주 두개라서. 전주부터 하려고 [0]을 넣음
#requests할때 url하고 헤더 같이 가져올거


print(response.text)


# 규칙을 보니까 text로 받아오면 안돼. json임. json으로 받아오도록 하자.
json.loads(response.text)


# 깔끔하게 정리가 되었음. 우리가 원하는 데이터는 items가 key로 딕셔너리로 묶여 있음


json.loads(response.text)['items']


elements = json.loads(response.text)['items']
elements[0]


def get_list(query, page):  #어떤 쿼리를 할건지 페이지는 몇번까지 받아올건지
    print('='*5, query, page, '='*5)
    url = "https://openapi.naver.com/v1/search/kin.json?display=100&query=" + query + "&start=" + str(page+1)
    response = requests.get(url, headers=headers)
    elements = json.loads(response.text)['items']

    for i, elm in enumerate(elements): #첫 번째 element에 대한 인덱스를 지정해주기 위해 enumerate를 사용
        title = elm['title'].replace("<b>", "").replace("</b>", "")
        title = unescape(title) # escape된 문자를 unescape문자로 변경 -> 꺽쇠 등의 이스케이프코드는 언이스케이프해줘
        link = elm['link'] #링크를 key로 해서 뽑음
        description = unescape(elm['description'].replace("<b>", "").replace("</b>", "")) #디스크립션도 위와 동일하게~
        # description = unescape(description) -> 불필요한 코드임
        
        print([query, (page*100)+(i+1), title, link, description, title+" "+description])

        with open(file_name, 'a', encoding='utf-8') as f: # overwrite 안되도록 add할 것('a')
            f.write( f'{query}\t{(page*100)+(i+1)}\t{title}\t{link}\t{description}\t{title+" "+description}\n')

    return

# 이렇게 함수 정의 완료~


#정의한 함수를 동작시키자
for query in queries:
    for page in range(goal_page):
        kin_list = get_list(query, page)
        time.sleep(0.5) #웹페이지 크롤링 매너 최소 6초



